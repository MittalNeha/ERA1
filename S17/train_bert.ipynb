{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:12:15.545542469Z",
     "start_time": "2023-09-15T19:12:13.738138029Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "from BERT import build_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    # Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "\n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e: i for i, e in enumerate(dataset.vocab)}\n",
    "        dataset.rvocab = {v: k for k, v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "\n",
    "        # special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>']  # replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>']  # replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>']  # replacement tag for the masked word prediction task\n",
    "\n",
    "    # fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "\n",
    "        # while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "\n",
    "        # ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))]  # PAD ok\n",
    "\n",
    "        # apply random mask\n",
    "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "\n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    # return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    # get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:12:15.557282869Z",
     "start_time": "2023-09-15T19:12:15.552929050Z"
    }
   },
   "id": "7a3c0c7da0d9165f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:12:15.561761279Z",
     "start_time": "2023-09-15T19:12:15.557873324Z"
    }
   },
   "id": "422fbd48e533182d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "# optimizer\n",
    "optim_kwargs = {'lr': 1e-4, 'weight_decay': 1e-4, 'betas': (.9, .999)}\n",
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "# 1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "# 2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "# 3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab)  # keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "# 4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle': True, 'drop_last': True, 'pin_memory': True, 'batch_size': batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:12:17.293343385Z",
     "start_time": "2023-09-15T19:12:15.566499239Z"
    }
   },
   "id": "9fb8839dd1f4c485"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 10.09  | Δw: 9.977\n",
      "it: 10  | loss 9.89  | Δw: 5.885\n",
      "it: 20  | loss 9.72  | Δw: 4.01\n",
      "it: 30  | loss 9.54  | Δw: 2.79\n",
      "it: 40  | loss 9.37  | Δw: 2.159\n",
      "it: 50  | loss 9.19  | Δw: 1.765\n",
      "it: 60  | loss 9.03  | Δw: 1.521\n",
      "it: 70  | loss 8.88  | Δw: 1.337\n",
      "it: 80  | loss 8.69  | Δw: 1.211\n",
      "it: 90  | loss 8.51  | Δw: 1.084\n",
      "it: 100  | loss 8.39  | Δw: 1.002\n",
      "it: 110  | loss 8.25  | Δw: 0.944\n",
      "it: 120  | loss 8.12  | Δw: 0.875\n",
      "it: 130  | loss 8.01  | Δw: 0.862\n",
      "it: 140  | loss 7.89  | Δw: 0.781\n",
      "it: 150  | loss 7.73  | Δw: 0.75\n",
      "it: 160  | loss 7.64  | Δw: 0.719\n",
      "it: 170  | loss 7.5  | Δw: 0.704\n",
      "it: 180  | loss 7.46  | Δw: 0.669\n",
      "it: 190  | loss 7.36  | Δw: 0.639\n",
      "it: 200  | loss 7.27  | Δw: 0.638\n",
      "it: 210  | loss 7.17  | Δw: 0.61\n",
      "it: 220  | loss 7.11  | Δw: 0.594\n",
      "it: 230  | loss 7.04  | Δw: 0.587\n",
      "it: 240  | loss 6.96  | Δw: 0.581\n",
      "it: 250  | loss 6.92  | Δw: 0.567\n",
      "it: 260  | loss 6.87  | Δw: 0.548\n",
      "it: 270  | loss 6.81  | Δw: 0.546\n",
      "it: 280  | loss 6.75  | Δw: 0.536\n",
      "it: 290  | loss 6.73  | Δw: 0.524\n",
      "it: 300  | loss 6.66  | Δw: 0.522\n",
      "it: 310  | loss 6.63  | Δw: 0.529\n",
      "it: 320  | loss 6.59  | Δw: 0.522\n",
      "it: 330  | loss 6.64  | Δw: 0.505\n",
      "it: 340  | loss 6.48  | Δw: 0.49\n",
      "it: 350  | loss 6.53  | Δw: 0.509\n",
      "it: 360  | loss 6.5  | Δw: 0.505\n",
      "it: 370  | loss 6.52  | Δw: 0.487\n",
      "it: 380  | loss 6.55  | Δw: 0.499\n",
      "it: 390  | loss 6.48  | Δw: 0.497\n",
      "it: 400  | loss 6.4  | Δw: 0.495\n",
      "it: 410  | loss 6.47  | Δw: 0.487\n",
      "it: 420  | loss 6.4  | Δw: 0.492\n",
      "it: 430  | loss 6.49  | Δw: 0.498\n",
      "it: 440  | loss 6.37  | Δw: 0.519\n",
      "it: 450  | loss 6.36  | Δw: 0.511\n",
      "it: 460  | loss 6.49  | Δw: 0.518\n",
      "it: 470  | loss 6.33  | Δw: 0.508\n",
      "it: 480  | loss 6.38  | Δw: 0.512\n",
      "it: 490  | loss 6.37  | Δw: 0.514\n",
      "it: 500  | loss 6.41  | Δw: 0.526\n",
      "it: 510  | loss 6.46  | Δw: 0.518\n",
      "it: 520  | loss 6.42  | Δw: 0.554\n",
      "it: 530  | loss 6.43  | Δw: 0.551\n",
      "it: 540  | loss 6.36  | Δw: 0.57\n",
      "it: 550  | loss 6.4  | Δw: 0.59\n",
      "it: 560  | loss 6.36  | Δw: 0.585\n",
      "it: 570  | loss 6.32  | Δw: 0.602\n",
      "it: 580  | loss 6.4  | Δw: 0.627\n",
      "it: 590  | loss 6.39  | Δw: 0.667\n",
      "it: 600  | loss 6.33  | Δw: 0.682\n",
      "it: 610  | loss 6.34  | Δw: 0.725\n",
      "it: 620  | loss 6.36  | Δw: 0.77\n",
      "it: 630  | loss 6.37  | Δw: 0.831\n",
      "it: 640  | loss 6.39  | Δw: 0.93\n",
      "it: 650  | loss 6.34  | Δw: 1.154\n",
      "it: 660  | loss 6.42  | Δw: 1.394\n",
      "it: 670  | loss 6.34  | Δw: 1.694\n",
      "it: 680  | loss 6.31  | Δw: 2.662\n",
      "it: 690  | loss 6.3  | Δw: 3.454\n",
      "it: 700  | loss 6.3  | Δw: 4.718\n",
      "it: 710  | loss 6.28  | Δw: 5.806\n",
      "it: 720  | loss 6.3  | Δw: 7.069\n",
      "it: 730  | loss 6.22  | Δw: 8.172\n",
      "it: 740  | loss 6.31  | Δw: 11.785\n",
      "it: 750  | loss 6.34  | Δw: 8.895\n",
      "it: 760  | loss 6.28  | Δw: 11.19\n",
      "it: 770  | loss 6.26  | Δw: 10.843\n",
      "it: 780  | loss 6.28  | Δw: 17.301\n",
      "it: 790  | loss 6.29  | Δw: 13.283\n",
      "it: 800  | loss 6.37  | Δw: 19.26\n",
      "it: 810  | loss 6.29  | Δw: 16.739\n",
      "it: 820  | loss 6.28  | Δw: 21.366\n",
      "it: 830  | loss 6.31  | Δw: 19.711\n",
      "it: 840  | loss 6.29  | Δw: 26.692\n",
      "it: 850  | loss 6.35  | Δw: 26.21\n",
      "it: 860  | loss 6.29  | Δw: 28.988\n",
      "it: 870  | loss 6.24  | Δw: 32.445\n",
      "it: 880  | loss 6.32  | Δw: 31.504\n",
      "it: 890  | loss 6.23  | Δw: 33.588\n",
      "it: 900  | loss 6.18  | Δw: 34.23\n",
      "it: 910  | loss 6.21  | Δw: 38.227\n",
      "it: 920  | loss 6.27  | Δw: 35.121\n",
      "it: 930  | loss 6.07  | Δw: 35.375\n",
      "it: 940  | loss 6.25  | Δw: 38.131\n",
      "it: 950  | loss 6.27  | Δw: 42.272\n",
      "it: 960  | loss 6.21  | Δw: 43.935\n",
      "it: 970  | loss 6.12  | Δw: 40.462\n",
      "it: 980  | loss 6.18  | Δw: 40.12\n",
      "it: 990  | loss 6.15  | Δw: 41.008\n",
      "it: 1000  | loss 6.15  | Δw: 44.893\n",
      "it: 1010  | loss 6.19  | Δw: 42.202\n",
      "it: 1020  | loss 6.07  | Δw: 45.618\n",
      "it: 1030  | loss 6.17  | Δw: 47.023\n",
      "it: 1040  | loss 6.15  | Δw: 47.324\n",
      "it: 1050  | loss 6.01  | Δw: 51.275\n",
      "it: 1060  | loss 6.0  | Δw: 54.278\n",
      "it: 1070  | loss 6.1  | Δw: 51.711\n",
      "it: 1080  | loss 6.0  | Δw: 53.307\n",
      "it: 1090  | loss 5.97  | Δw: 54.021\n",
      "it: 1100  | loss 6.06  | Δw: 55.071\n",
      "it: 1110  | loss 6.0  | Δw: 60.011\n",
      "it: 1120  | loss 6.0  | Δw: 58.433\n",
      "it: 1130  | loss 5.97  | Δw: 59.86\n",
      "it: 1140  | loss 5.97  | Δw: 62.079\n",
      "it: 1150  | loss 5.96  | Δw: 62.888\n",
      "it: 1160  | loss 5.97  | Δw: 65.674\n",
      "it: 1170  | loss 5.99  | Δw: 66.114\n",
      "it: 1180  | loss 5.95  | Δw: 71.221\n",
      "it: 1190  | loss 5.94  | Δw: 68.48\n",
      "it: 1200  | loss 5.87  | Δw: 71.961\n",
      "it: 1210  | loss 5.83  | Δw: 75.011\n",
      "it: 1220  | loss 5.91  | Δw: 72.522\n",
      "it: 1230  | loss 5.9  | Δw: 77.786\n",
      "it: 1240  | loss 5.85  | Δw: 83.355\n",
      "it: 1250  | loss 5.85  | Δw: 78.776\n",
      "it: 1260  | loss 5.78  | Δw: 80.321\n",
      "it: 1270  | loss 5.74  | Δw: 80.972\n",
      "it: 1280  | loss 5.84  | Δw: 80.012\n",
      "it: 1290  | loss 5.66  | Δw: 89.677\n",
      "it: 1300  | loss 5.79  | Δw: 86.298\n",
      "it: 1310  | loss 5.75  | Δw: 90.358\n",
      "it: 1320  | loss 5.71  | Δw: 89.393\n",
      "it: 1330  | loss 5.75  | Δw: 93.016\n",
      "it: 1340  | loss 5.68  | Δw: 97.359\n",
      "it: 1350  | loss 5.72  | Δw: 96.476\n",
      "it: 1360  | loss 5.68  | Δw: 96.009\n",
      "it: 1370  | loss 5.74  | Δw: 96.314\n",
      "it: 1380  | loss 5.64  | Δw: 102.477\n",
      "it: 1390  | loss 5.6  | Δw: 99.096\n",
      "it: 1400  | loss 5.69  | Δw: 101.071\n",
      "it: 1410  | loss 5.73  | Δw: 103.018\n",
      "it: 1420  | loss 5.64  | Δw: 106.589\n",
      "it: 1430  | loss 5.7  | Δw: 107.536\n",
      "it: 1440  | loss 5.63  | Δw: 103.054\n",
      "it: 1450  | loss 5.66  | Δw: 104.509\n",
      "it: 1460  | loss 5.6  | Δw: 108.078\n",
      "it: 1470  | loss 5.67  | Δw: 112.59\n",
      "it: 1480  | loss 5.71  | Δw: 113.599\n",
      "it: 1490  | loss 5.62  | Δw: 114.319\n",
      "it: 1500  | loss 5.57  | Δw: 116.031\n",
      "it: 1510  | loss 5.52  | Δw: 116.817\n",
      "it: 1520  | loss 5.68  | Δw: 113.27\n",
      "it: 1530  | loss 5.54  | Δw: 118.042\n",
      "it: 1540  | loss 5.46  | Δw: 121.077\n",
      "it: 1550  | loss 5.57  | Δw: 122.786\n",
      "it: 1560  | loss 5.5  | Δw: 121.761\n",
      "it: 1570  | loss 5.6  | Δw: 116.212\n",
      "it: 1580  | loss 5.44  | Δw: 120.55\n",
      "it: 1590  | loss 5.49  | Δw: 123.693\n",
      "it: 1600  | loss 5.57  | Δw: 115.217\n",
      "it: 1610  | loss 5.5  | Δw: 127.041\n",
      "it: 1620  | loss 5.48  | Δw: 132.295\n",
      "it: 1630  | loss 5.47  | Δw: 130.905\n",
      "it: 1640  | loss 5.45  | Δw: 127.18\n",
      "it: 1650  | loss 5.44  | Δw: 128.887\n",
      "it: 1660  | loss 5.39  | Δw: 138.69\n",
      "it: 1670  | loss 5.45  | Δw: 131.397\n",
      "it: 1680  | loss 5.5  | Δw: 132.97\n",
      "it: 1690  | loss 5.36  | Δw: 135.202\n",
      "it: 1700  | loss 5.39  | Δw: 143.958\n",
      "it: 1710  | loss 5.6  | Δw: 137.204\n",
      "it: 1720  | loss 5.47  | Δw: 134.539\n",
      "it: 1730  | loss 5.35  | Δw: 139.124\n",
      "it: 1740  | loss 5.51  | Δw: 136.942\n",
      "it: 1750  | loss 5.38  | Δw: 141.078\n",
      "it: 1760  | loss 5.4  | Δw: 147.381\n",
      "it: 1770  | loss 5.42  | Δw: 144.071\n",
      "it: 1780  | loss 5.33  | Δw: 140.224\n",
      "it: 1790  | loss 5.38  | Δw: 138.763\n",
      "it: 1800  | loss 5.34  | Δw: 147.178\n",
      "it: 1810  | loss 5.29  | Δw: 143.715\n",
      "it: 1820  | loss 5.35  | Δw: 144.754\n",
      "it: 1830  | loss 5.22  | Δw: 149.861\n",
      "it: 1840  | loss 5.23  | Δw: 148.541\n",
      "it: 1850  | loss 5.2  | Δw: 148.572\n",
      "it: 1860  | loss 5.27  | Δw: 147.943\n",
      "it: 1870  | loss 5.29  | Δw: 145.37\n",
      "it: 1880  | loss 5.33  | Δw: 152.152\n",
      "it: 1890  | loss 5.34  | Δw: 146.263\n",
      "it: 1900  | loss 5.29  | Δw: 146.522\n",
      "it: 1910  | loss 5.26  | Δw: 155.652\n",
      "it: 1920  | loss 5.29  | Δw: 156.806\n",
      "it: 1930  | loss 5.25  | Δw: 154.285\n",
      "it: 1940  | loss 5.19  | Δw: 155.069\n",
      "it: 1950  | loss 5.26  | Δw: 158.308\n",
      "it: 1960  | loss 5.29  | Δw: 156.635\n",
      "it: 1970  | loss 5.18  | Δw: 157.156\n",
      "it: 1980  | loss 5.24  | Δw: 159.644\n",
      "it: 1990  | loss 5.17  | Δw: 147.939\n",
      "it: 2000  | loss 5.26  | Δw: 159.191\n",
      "it: 2010  | loss 5.23  | Δw: 162.303\n",
      "it: 2020  | loss 5.2  | Δw: 161.579\n",
      "it: 2030  | loss 5.23  | Δw: 161.448\n",
      "it: 2040  | loss 5.1  | Δw: 157.621\n",
      "it: 2050  | loss 5.13  | Δw: 158.758\n",
      "it: 2060  | loss 5.29  | Δw: 154.202\n",
      "it: 2070  | loss 5.26  | Δw: 164.565\n",
      "it: 2080  | loss 5.22  | Δw: 164.216\n",
      "it: 2090  | loss 5.11  | Δw: 166.803\n",
      "it: 2100  | loss 5.14  | Δw: 169.705\n",
      "it: 2110  | loss 5.13  | Δw: 165.804\n",
      "it: 2120  | loss 5.11  | Δw: 167.459\n",
      "it: 2130  | loss 5.17  | Δw: 164.813\n",
      "it: 2140  | loss 5.2  | Δw: 163.261\n",
      "it: 2150  | loss 5.15  | Δw: 176.646\n",
      "it: 2160  | loss 5.2  | Δw: 167.239\n",
      "it: 2170  | loss 5.07  | Δw: 174.876\n",
      "it: 2180  | loss 5.16  | Δw: 169.733\n",
      "it: 2190  | loss 5.04  | Δw: 168.773\n",
      "it: 2200  | loss 5.14  | Δw: 166.615\n",
      "it: 2210  | loss 5.18  | Δw: 176.994\n",
      "it: 2220  | loss 5.16  | Δw: 171.428\n",
      "it: 2230  | loss 5.11  | Δw: 176.417\n",
      "it: 2240  | loss 5.06  | Δw: 166.045\n",
      "it: 2250  | loss 5.12  | Δw: 176.998\n",
      "it: 2260  | loss 5.0  | Δw: 171.601\n",
      "it: 2270  | loss 4.99  | Δw: 172.067\n",
      "it: 2280  | loss 5.11  | Δw: 165.033\n",
      "it: 2290  | loss 5.01  | Δw: 174.497\n",
      "it: 2300  | loss 5.02  | Δw: 178.761\n",
      "it: 2310  | loss 4.98  | Δw: 173.439\n",
      "it: 2320  | loss 4.96  | Δw: 178.882\n",
      "it: 2330  | loss 5.05  | Δw: 181.329\n",
      "it: 2340  | loss 4.98  | Δw: 177.791\n",
      "it: 2350  | loss 5.01  | Δw: 172.093\n",
      "it: 2360  | loss 4.96  | Δw: 185.09\n",
      "it: 2370  | loss 4.97  | Δw: 186.908\n",
      "it: 2380  | loss 5.1  | Δw: 172.653\n",
      "it: 2390  | loss 4.92  | Δw: 190.468\n",
      "it: 2400  | loss 5.0  | Δw: 177.41\n",
      "it: 2410  | loss 4.97  | Δw: 182.888\n",
      "it: 2420  | loss 4.94  | Δw: 183.983\n",
      "it: 2430  | loss 4.97  | Δw: 177.485\n",
      "it: 2440  | loss 4.97  | Δw: 180.479\n",
      "it: 2450  | loss 4.9  | Δw: 184.344\n",
      "it: 2460  | loss 5.05  | Δw: 183.549\n",
      "it: 2470  | loss 5.01  | Δw: 186.602\n",
      "it: 2480  | loss 4.95  | Δw: 189.027\n",
      "it: 2490  | loss 4.99  | Δw: 186.881\n",
      "it: 2500  | loss 5.05  | Δw: 188.68\n",
      "it: 2510  | loss 4.87  | Δw: 190.186\n",
      "it: 2520  | loss 5.07  | Δw: 186.925\n",
      "it: 2530  | loss 4.95  | Δw: 190.041\n",
      "it: 2540  | loss 5.01  | Δw: 187.514\n",
      "it: 2550  | loss 4.93  | Δw: 186.687\n",
      "it: 2560  | loss 4.98  | Δw: 181.36\n",
      "it: 2570  | loss 5.01  | Δw: 184.582\n",
      "it: 2580  | loss 4.93  | Δw: 187.629\n",
      "it: 2590  | loss 4.95  | Δw: 184.13\n",
      "it: 2600  | loss 4.85  | Δw: 191.523\n",
      "it: 2610  | loss 4.84  | Δw: 195.767\n",
      "it: 2620  | loss 4.95  | Δw: 194.445\n",
      "it: 2630  | loss 4.87  | Δw: 183.456\n",
      "it: 2640  | loss 4.96  | Δw: 188.45\n",
      "it: 2650  | loss 4.8  | Δw: 201.005\n",
      "it: 2660  | loss 4.91  | Δw: 195.655\n",
      "it: 2670  | loss 4.86  | Δw: 195.9\n",
      "it: 2680  | loss 4.82  | Δw: 198.168\n",
      "it: 2690  | loss 4.79  | Δw: 196.739\n",
      "it: 2700  | loss 4.88  | Δw: 193.755\n",
      "it: 2710  | loss 4.71  | Δw: 195.93\n",
      "it: 2720  | loss 4.91  | Δw: 194.529\n",
      "it: 2730  | loss 4.88  | Δw: 198.146\n",
      "it: 2740  | loss 4.82  | Δw: 194.692\n",
      "it: 2750  | loss 5.0  | Δw: 192.705\n",
      "it: 2760  | loss 4.88  | Δw: 189.835\n",
      "it: 2770  | loss 4.78  | Δw: 193.744\n",
      "it: 2780  | loss 4.86  | Δw: 200.89\n",
      "it: 2790  | loss 4.74  | Δw: 201.51\n",
      "it: 2800  | loss 4.75  | Δw: 196.689\n",
      "it: 2810  | loss 4.81  | Δw: 198.819\n",
      "it: 2820  | loss 4.8  | Δw: 203.725\n",
      "it: 2830  | loss 5.0  | Δw: 199.378\n",
      "it: 2840  | loss 4.8  | Δw: 194.069\n",
      "it: 2850  | loss 4.87  | Δw: 195.574\n",
      "it: 2860  | loss 4.77  | Δw: 189.929\n",
      "it: 2870  | loss 4.81  | Δw: 209.998\n",
      "it: 2880  | loss 4.83  | Δw: 199.145\n",
      "it: 2890  | loss 4.79  | Δw: 202.708\n",
      "it: 2900  | loss 4.84  | Δw: 197.565\n",
      "it: 2910  | loss 4.8  | Δw: 200.868\n",
      "it: 2920  | loss 4.78  | Δw: 210.911\n",
      "it: 2930  | loss 4.77  | Δw: 204.189\n",
      "it: 2940  | loss 4.75  | Δw: 206.12\n",
      "it: 2950  | loss 4.76  | Δw: 211.858\n",
      "it: 2960  | loss 4.69  | Δw: 204.94\n",
      "it: 2970  | loss 4.68  | Δw: 201.422\n",
      "it: 2980  | loss 4.66  | Δw: 214.343\n",
      "it: 2990  | loss 4.73  | Δw: 210.145\n",
      "it: 3000  | loss 4.79  | Δw: 207.969\n",
      "it: 3010  | loss 4.78  | Δw: 196.405\n",
      "it: 3020  | loss 4.76  | Δw: 201.72\n",
      "it: 3030  | loss 4.79  | Δw: 207.669\n",
      "it: 3040  | loss 4.73  | Δw: 213.247\n",
      "it: 3050  | loss 4.69  | Δw: 207.454\n",
      "it: 3060  | loss 4.74  | Δw: 202.852\n",
      "it: 3070  | loss 4.82  | Δw: 204.74\n",
      "it: 3080  | loss 4.78  | Δw: 207.889\n",
      "it: 3090  | loss 4.77  | Δw: 211.42\n",
      "it: 3100  | loss 4.64  | Δw: 211.606\n",
      "it: 3110  | loss 4.7  | Δw: 211.563\n",
      "it: 3120  | loss 4.78  | Δw: 203.953\n",
      "it: 3130  | loss 4.59  | Δw: 210.018\n",
      "it: 3140  | loss 4.76  | Δw: 208.993\n",
      "it: 3150  | loss 4.68  | Δw: 204.892\n",
      "it: 3160  | loss 4.76  | Δw: 209.707\n",
      "it: 3170  | loss 4.65  | Δw: 213.433\n",
      "it: 3180  | loss 4.58  | Δw: 207.538\n",
      "it: 3190  | loss 4.74  | Δw: 211.063\n",
      "it: 3200  | loss 4.69  | Δw: 212.199\n",
      "it: 3210  | loss 4.73  | Δw: 204.401\n",
      "it: 3220  | loss 4.63  | Δw: 218.842\n",
      "it: 3230  | loss 4.64  | Δw: 217.132\n",
      "it: 3240  | loss 4.73  | Δw: 214.196\n",
      "it: 3250  | loss 4.67  | Δw: 219.061\n",
      "it: 3260  | loss 4.73  | Δw: 210.731\n",
      "it: 3270  | loss 4.69  | Δw: 216.835\n",
      "it: 3280  | loss 4.7  | Δw: 215.762\n",
      "it: 3290  | loss 4.51  | Δw: 211.076\n",
      "it: 3300  | loss 4.67  | Δw: 210.518\n",
      "it: 3310  | loss 4.73  | Δw: 213.575\n",
      "it: 3320  | loss 4.67  | Δw: 215.292\n",
      "it: 3330  | loss 4.69  | Δw: 205.68\n",
      "it: 3340  | loss 4.64  | Δw: 214.196\n",
      "it: 3350  | loss 4.61  | Δw: 215.292\n",
      "it: 3360  | loss 4.62  | Δw: 216.684\n",
      "it: 3370  | loss 4.57  | Δw: 216.047\n",
      "it: 3380  | loss 4.55  | Δw: 211.28\n",
      "it: 3390  | loss 4.69  | Δw: 214.177\n",
      "it: 3400  | loss 4.66  | Δw: 213.084\n",
      "it: 3410  | loss 4.67  | Δw: 223.853\n",
      "it: 3420  | loss 4.59  | Δw: 219.478\n",
      "it: 3430  | loss 4.57  | Δw: 218.145\n",
      "it: 3440  | loss 4.6  | Δw: 220.419\n",
      "it: 3450  | loss 4.56  | Δw: 221.767\n",
      "it: 3460  | loss 4.71  | Δw: 224.912\n",
      "it: 3470  | loss 4.6  | Δw: 221.412\n",
      "it: 3480  | loss 4.61  | Δw: 231.285\n",
      "it: 3490  | loss 4.59  | Δw: 219.445\n",
      "it: 3500  | loss 4.64  | Δw: 213.907\n",
      "it: 3510  | loss 4.65  | Δw: 217.811\n",
      "it: 3520  | loss 4.75  | Δw: 218.436\n",
      "it: 3530  | loss 4.65  | Δw: 215.967\n",
      "it: 3540  | loss 4.61  | Δw: 220.064\n",
      "it: 3550  | loss 4.69  | Δw: 222.111\n",
      "it: 3560  | loss 4.53  | Δw: 224.772\n",
      "it: 3570  | loss 4.57  | Δw: 223.155\n",
      "it: 3580  | loss 4.6  | Δw: 224.954\n",
      "it: 3590  | loss 4.56  | Δw: 217.367\n",
      "it: 3600  | loss 4.44  | Δw: 220.541\n",
      "it: 3610  | loss 4.62  | Δw: 218.089\n",
      "it: 3620  | loss 4.62  | Δw: 222.076\n",
      "it: 3630  | loss 4.53  | Δw: 218.09\n",
      "it: 3640  | loss 4.6  | Δw: 225.281\n",
      "it: 3650  | loss 4.65  | Δw: 231.297\n",
      "it: 3660  | loss 4.48  | Δw: 219.461\n",
      "it: 3670  | loss 4.59  | Δw: 218.563\n",
      "it: 3680  | loss 4.49  | Δw: 216.471\n",
      "it: 3690  | loss 4.48  | Δw: 221.282\n",
      "it: 3700  | loss 4.58  | Δw: 231.897\n",
      "it: 3710  | loss 4.48  | Δw: 231.102\n",
      "it: 3720  | loss 4.51  | Δw: 236.301\n",
      "it: 3730  | loss 4.55  | Δw: 225.693\n",
      "it: 3740  | loss 4.58  | Δw: 226.779\n",
      "it: 3750  | loss 4.59  | Δw: 228.141\n",
      "it: 3760  | loss 4.54  | Δw: 231.03\n",
      "it: 3770  | loss 4.55  | Δw: 227.129\n",
      "it: 3780  | loss 4.55  | Δw: 235.744\n",
      "it: 3790  | loss 4.48  | Δw: 217.111\n",
      "it: 3800  | loss 4.51  | Δw: 232.051\n",
      "it: 3810  | loss 4.53  | Δw: 219.831\n",
      "it: 3820  | loss 4.52  | Δw: 222.431\n",
      "it: 3830  | loss 4.55  | Δw: 226.154\n",
      "it: 3840  | loss 4.5  | Δw: 221.872\n",
      "it: 3850  | loss 4.53  | Δw: 226.549\n",
      "it: 3860  | loss 4.58  | Δw: 227.281\n",
      "it: 3870  | loss 4.59  | Δw: 228.143\n",
      "it: 3880  | loss 4.49  | Δw: 232.999\n",
      "it: 3890  | loss 4.61  | Δw: 232.399\n",
      "it: 3900  | loss 4.56  | Δw: 219.691\n",
      "it: 3910  | loss 4.36  | Δw: 227.852\n",
      "it: 3920  | loss 4.58  | Δw: 235.514\n",
      "it: 3930  | loss 4.53  | Δw: 234.179\n",
      "it: 3940  | loss 4.49  | Δw: 230.377\n",
      "it: 3950  | loss 4.53  | Δw: 229.039\n",
      "it: 3960  | loss 4.46  | Δw: 222.721\n",
      "it: 3970  | loss 4.62  | Δw: 238.852\n",
      "it: 3980  | loss 4.5  | Δw: 235.846\n",
      "it: 3990  | loss 4.38  | Δw: 225.579\n",
      "it: 4000  | loss 4.59  | Δw: 230.495\n",
      "it: 4010  | loss 4.56  | Δw: 226.918\n",
      "it: 4020  | loss 4.52  | Δw: 230.962\n",
      "it: 4030  | loss 4.49  | Δw: 233.629\n",
      "it: 4040  | loss 4.47  | Δw: 228.254\n",
      "it: 4050  | loss 4.57  | Δw: 231.458\n",
      "it: 4060  | loss 4.55  | Δw: 222.615\n",
      "it: 4070  | loss 4.55  | Δw: 223.579\n",
      "it: 4080  | loss 4.39  | Δw: 232.596\n",
      "it: 4090  | loss 4.38  | Δw: 227.619\n",
      "it: 4100  | loss 4.44  | Δw: 244.275\n",
      "it: 4110  | loss 4.52  | Δw: 231.304\n",
      "it: 4120  | loss 4.41  | Δw: 230.376\n",
      "it: 4130  | loss 4.5  | Δw: 233.132\n",
      "it: 4140  | loss 4.43  | Δw: 228.641\n",
      "it: 4150  | loss 4.58  | Δw: 228.523\n",
      "it: 4160  | loss 4.5  | Δw: 236.498\n",
      "it: 4170  | loss 4.51  | Δw: 234.471\n",
      "it: 4180  | loss 4.55  | Δw: 237.897\n",
      "it: 4190  | loss 4.48  | Δw: 236.439\n",
      "it: 4200  | loss 4.38  | Δw: 230.504\n",
      "it: 4210  | loss 4.44  | Δw: 242.294\n",
      "it: 4220  | loss 4.42  | Δw: 233.745\n",
      "it: 4230  | loss 4.48  | Δw: 238.342\n",
      "it: 4240  | loss 4.46  | Δw: 230.482\n",
      "it: 4250  | loss 4.45  | Δw: 237.228\n",
      "it: 4260  | loss 4.45  | Δw: 236.571\n",
      "it: 4270  | loss 4.46  | Δw: 241.51\n",
      "it: 4280  | loss 4.49  | Δw: 231.484\n",
      "it: 4290  | loss 4.45  | Δw: 236.055\n",
      "it: 4300  | loss 4.55  | Δw: 233.88\n",
      "it: 4310  | loss 4.37  | Δw: 234.057\n",
      "it: 4320  | loss 4.33  | Δw: 237.141\n",
      "it: 4330  | loss 4.48  | Δw: 233.942\n",
      "it: 4340  | loss 4.41  | Δw: 233.791\n",
      "it: 4350  | loss 4.57  | Δw: 227.429\n",
      "it: 4360  | loss 4.39  | Δw: 237.616\n",
      "it: 4370  | loss 4.37  | Δw: 231.698\n",
      "it: 4380  | loss 4.42  | Δw: 238.452\n",
      "it: 4390  | loss 4.46  | Δw: 235.176\n",
      "it: 4400  | loss 4.47  | Δw: 235.447\n",
      "it: 4410  | loss 4.46  | Δw: 248.373\n",
      "it: 4420  | loss 4.37  | Δw: 233.205\n",
      "it: 4430  | loss 4.49  | Δw: 238.206\n",
      "it: 4440  | loss 4.58  | Δw: 236.709\n",
      "it: 4450  | loss 4.43  | Δw: 232.414\n",
      "it: 4460  | loss 4.34  | Δw: 246.656\n",
      "it: 4470  | loss 4.36  | Δw: 229.775\n",
      "it: 4480  | loss 4.51  | Δw: 244.314\n",
      "it: 4490  | loss 4.56  | Δw: 241.421\n",
      "it: 4500  | loss 4.47  | Δw: 239.716\n",
      "it: 4510  | loss 4.38  | Δw: 237.135\n",
      "it: 4520  | loss 4.48  | Δw: 243.915\n",
      "it: 4530  | loss 4.53  | Δw: 232.675\n",
      "it: 4540  | loss 4.47  | Δw: 235.425\n",
      "it: 4550  | loss 4.46  | Δw: 243.866\n",
      "it: 4560  | loss 4.45  | Δw: 241.797\n",
      "it: 4570  | loss 4.29  | Δw: 245.62\n",
      "it: 4580  | loss 4.42  | Δw: 243.178\n",
      "it: 4590  | loss 4.41  | Δw: 232.782\n",
      "it: 4600  | loss 4.46  | Δw: 236.712\n",
      "it: 4610  | loss 4.46  | Δw: 241.044\n",
      "it: 4620  | loss 4.3  | Δw: 244.235\n",
      "it: 4630  | loss 4.42  | Δw: 242.07\n",
      "it: 4640  | loss 4.38  | Δw: 239.624\n",
      "it: 4650  | loss 4.32  | Δw: 238.521\n",
      "it: 4660  | loss 4.43  | Δw: 235.197\n",
      "it: 4670  | loss 4.5  | Δw: 243.921\n",
      "it: 4680  | loss 4.41  | Δw: 242.229\n",
      "it: 4690  | loss 4.4  | Δw: 244.071\n",
      "it: 4700  | loss 4.36  | Δw: 242.665\n",
      "it: 4710  | loss 4.44  | Δw: 237.456\n",
      "it: 4720  | loss 4.34  | Δw: 235.413\n",
      "it: 4730  | loss 4.31  | Δw: 251.276\n",
      "it: 4740  | loss 4.22  | Δw: 241.808\n",
      "it: 4750  | loss 4.34  | Δw: 249.228\n",
      "it: 4760  | loss 4.3  | Δw: 250.656\n",
      "it: 4770  | loss 4.25  | Δw: 235.873\n",
      "it: 4780  | loss 4.44  | Δw: 238.166\n",
      "it: 4790  | loss 4.24  | Δw: 244.526\n",
      "it: 4800  | loss 4.39  | Δw: 237.099\n",
      "it: 4810  | loss 4.33  | Δw: 240.922\n",
      "it: 4820  | loss 4.32  | Δw: 238.923\n",
      "it: 4830  | loss 4.34  | Δw: 251.461\n",
      "it: 4840  | loss 4.41  | Δw: 237.724\n",
      "it: 4850  | loss 4.39  | Δw: 241.906\n",
      "it: 4860  | loss 4.45  | Δw: 248.469\n",
      "it: 4870  | loss 4.43  | Δw: 239.415\n",
      "it: 4880  | loss 4.32  | Δw: 240.473\n",
      "it: 4890  | loss 4.4  | Δw: 244.252\n",
      "it: 4900  | loss 4.39  | Δw: 246.226\n",
      "it: 4910  | loss 4.35  | Δw: 245.506\n",
      "it: 4920  | loss 4.35  | Δw: 250.573\n",
      "it: 4930  | loss 4.4  | Δw: 247.256\n",
      "it: 4940  | loss 4.32  | Δw: 248.391\n",
      "it: 4950  | loss 4.3  | Δw: 252.062\n",
      "it: 4960  | loss 4.33  | Δw: 244.584\n",
      "it: 4970  | loss 4.35  | Δw: 240.498\n",
      "it: 4980  | loss 4.28  | Δw: 251.912\n",
      "it: 4990  | loss 4.31  | Δw: 251.687\n",
      "it: 5000  | loss 4.31  | Δw: 253.597\n",
      "it: 5010  | loss 4.26  | Δw: 245.867\n",
      "it: 5020  | loss 4.39  | Δw: 236.978\n",
      "it: 5030  | loss 4.38  | Δw: 251.534\n",
      "it: 5040  | loss 4.31  | Δw: 247.564\n",
      "it: 5050  | loss 4.38  | Δw: 250.308\n",
      "it: 5060  | loss 4.35  | Δw: 256.489\n",
      "it: 5070  | loss 4.34  | Δw: 238.701\n",
      "it: 5080  | loss 4.38  | Δw: 240.256\n",
      "it: 5090  | loss 4.3  | Δw: 244.783\n",
      "it: 5100  | loss 4.37  | Δw: 249.705\n",
      "it: 5110  | loss 4.33  | Δw: 247.509\n",
      "it: 5120  | loss 4.26  | Δw: 249.54\n",
      "it: 5130  | loss 4.29  | Δw: 252.741\n",
      "it: 5140  | loss 4.31  | Δw: 238.64\n",
      "it: 5150  | loss 4.39  | Δw: 245.223\n",
      "it: 5160  | loss 4.26  | Δw: 257.049\n",
      "it: 5170  | loss 4.2  | Δw: 243.347\n",
      "it: 5180  | loss 4.42  | Δw: 254.056\n",
      "it: 5190  | loss 4.27  | Δw: 244.942\n",
      "it: 5200  | loss 4.36  | Δw: 245.813\n",
      "it: 5210  | loss 4.19  | Δw: 252.705\n",
      "it: 5220  | loss 4.27  | Δw: 258.594\n",
      "it: 5230  | loss 4.27  | Δw: 248.928\n",
      "it: 5240  | loss 4.35  | Δw: 255.461\n",
      "it: 5250  | loss 4.28  | Δw: 256.474\n",
      "it: 5260  | loss 4.18  | Δw: 242.785\n",
      "it: 5270  | loss 4.34  | Δw: 244.859\n",
      "it: 5280  | loss 4.29  | Δw: 251.59\n",
      "it: 5290  | loss 4.28  | Δw: 250.791\n",
      "it: 5300  | loss 4.32  | Δw: 257.402\n",
      "it: 5310  | loss 4.23  | Δw: 245.313\n",
      "it: 5320  | loss 4.32  | Δw: 252.524\n",
      "it: 5330  | loss 4.25  | Δw: 256.404\n",
      "it: 5340  | loss 4.34  | Δw: 253.254\n",
      "it: 5350  | loss 4.31  | Δw: 260.284\n",
      "it: 5360  | loss 4.26  | Δw: 252.661\n",
      "it: 5370  | loss 4.23  | Δw: 253.026\n",
      "it: 5380  | loss 4.31  | Δw: 256.539\n",
      "it: 5390  | loss 4.28  | Δw: 250.445\n",
      "it: 5400  | loss 4.21  | Δw: 266.917\n",
      "it: 5410  | loss 4.23  | Δw: 248.868\n",
      "it: 5420  | loss 4.24  | Δw: 244.595\n",
      "it: 5430  | loss 4.27  | Δw: 266.178\n",
      "it: 5440  | loss 4.22  | Δw: 252.023\n",
      "it: 5450  | loss 4.33  | Δw: 259.507\n",
      "it: 5460  | loss 4.29  | Δw: 249.732\n",
      "it: 5470  | loss 4.29  | Δw: 251.254\n",
      "it: 5480  | loss 4.35  | Δw: 256.157\n",
      "it: 5490  | loss 4.37  | Δw: 253.27\n",
      "it: 5500  | loss 4.2  | Δw: 253.721\n",
      "it: 5510  | loss 4.31  | Δw: 252.349\n",
      "it: 5520  | loss 4.34  | Δw: 247.399\n",
      "it: 5530  | loss 4.32  | Δw: 256.662\n",
      "it: 5540  | loss 4.26  | Δw: 260.136\n",
      "it: 5550  | loss 4.37  | Δw: 254.47\n",
      "it: 5560  | loss 4.23  | Δw: 246.154\n",
      "it: 5570  | loss 4.28  | Δw: 264.927\n",
      "it: 5580  | loss 4.33  | Δw: 256.804\n",
      "it: 5590  | loss 4.3  | Δw: 257.924\n",
      "it: 5600  | loss 4.29  | Δw: 253.251\n",
      "it: 5610  | loss 4.23  | Δw: 254.09\n",
      "it: 5620  | loss 4.22  | Δw: 254.024\n",
      "it: 5630  | loss 4.19  | Δw: 251.551\n",
      "it: 5640  | loss 4.1  | Δw: 254.509\n",
      "it: 5650  | loss 4.18  | Δw: 261.757\n",
      "it: 5660  | loss 4.35  | Δw: 264.244\n",
      "it: 5670  | loss 4.33  | Δw: 260.593\n",
      "it: 5680  | loss 4.22  | Δw: 269.954\n",
      "it: 5690  | loss 4.27  | Δw: 258.269\n",
      "it: 5700  | loss 4.28  | Δw: 248.415\n",
      "it: 5710  | loss 4.29  | Δw: 261.669\n",
      "it: 5720  | loss 4.17  | Δw: 250.707\n",
      "it: 5730  | loss 4.25  | Δw: 251.189\n",
      "it: 5740  | loss 4.14  | Δw: 260.778\n",
      "it: 5750  | loss 4.24  | Δw: 262.0\n",
      "it: 5760  | loss 4.19  | Δw: 240.873\n",
      "it: 5770  | loss 4.13  | Δw: 251.676\n",
      "it: 5780  | loss 4.25  | Δw: 261.141\n",
      "it: 5790  | loss 4.28  | Δw: 257.38\n",
      "it: 5800  | loss 4.1  | Δw: 263.079\n",
      "it: 5810  | loss 4.14  | Δw: 253.914\n",
      "it: 5820  | loss 4.23  | Δw: 263.075\n",
      "it: 5830  | loss 4.24  | Δw: 256.898\n",
      "it: 5840  | loss 4.27  | Δw: 272.612\n",
      "it: 5850  | loss 4.25  | Δw: 261.19\n",
      "it: 5860  | loss 4.15  | Δw: 260.515\n",
      "it: 5870  | loss 4.22  | Δw: 255.115\n",
      "it: 5880  | loss 4.16  | Δw: 261.734\n",
      "it: 5890  | loss 4.22  | Δw: 258.438\n",
      "it: 5900  | loss 4.21  | Δw: 259.28\n",
      "it: 5910  | loss 4.24  | Δw: 263.269\n",
      "it: 5920  | loss 4.19  | Δw: 254.933\n",
      "it: 5930  | loss 4.26  | Δw: 264.52\n",
      "it: 5940  | loss 4.13  | Δw: 256.205\n",
      "it: 5950  | loss 4.22  | Δw: 254.233\n",
      "it: 5960  | loss 4.18  | Δw: 257.455\n",
      "it: 5970  | loss 4.23  | Δw: 266.561\n",
      "it: 5980  | loss 4.33  | Δw: 267.23\n",
      "it: 5990  | loss 4.19  | Δw: 261.284\n",
      "it: 6000  | loss 4.12  | Δw: 258.877\n",
      "it: 6010  | loss 4.17  | Δw: 263.787\n",
      "it: 6020  | loss 4.21  | Δw: 265.643\n",
      "it: 6030  | loss 4.18  | Δw: 262.232\n",
      "it: 6040  | loss 4.31  | Δw: 264.671\n",
      "it: 6050  | loss 4.16  | Δw: 258.611\n",
      "it: 6060  | loss 4.2  | Δw: 265.284\n",
      "it: 6070  | loss 4.25  | Δw: 264.274\n",
      "it: 6080  | loss 4.31  | Δw: 259.011\n",
      "it: 6090  | loss 4.23  | Δw: 258.334\n",
      "it: 6100  | loss 4.16  | Δw: 260.993\n",
      "it: 6110  | loss 4.26  | Δw: 264.936\n",
      "it: 6120  | loss 4.25  | Δw: 262.315\n",
      "it: 6130  | loss 4.13  | Δw: 262.722\n",
      "it: 6140  | loss 4.21  | Δw: 267.825\n",
      "it: 6150  | loss 4.18  | Δw: 264.547\n",
      "it: 6160  | loss 4.15  | Δw: 263.94\n",
      "it: 6170  | loss 4.27  | Δw: 264.114\n",
      "it: 6180  | loss 4.06  | Δw: 261.061\n",
      "it: 6190  | loss 4.14  | Δw: 270.494\n",
      "it: 6200  | loss 4.15  | Δw: 267.699\n",
      "it: 6210  | loss 4.15  | Δw: 262.196\n",
      "it: 6220  | loss 4.21  | Δw: 260.432\n",
      "it: 6230  | loss 4.12  | Δw: 264.709\n",
      "it: 6240  | loss 4.21  | Δw: 261.821\n",
      "it: 6250  | loss 4.16  | Δw: 271.044\n",
      "it: 6260  | loss 4.1  | Δw: 271.965\n",
      "it: 6270  | loss 4.16  | Δw: 266.549\n",
      "it: 6280  | loss 4.1  | Δw: 274.947\n",
      "it: 6290  | loss 4.1  | Δw: 264.81\n",
      "it: 6300  | loss 4.21  | Δw: 262.188\n",
      "it: 6310  | loss 4.17  | Δw: 261.064\n",
      "it: 6320  | loss 4.2  | Δw: 268.891\n",
      "it: 6330  | loss 4.08  | Δw: 261.275\n",
      "it: 6340  | loss 4.13  | Δw: 260.164\n",
      "it: 6350  | loss 4.11  | Δw: 272.246\n",
      "it: 6360  | loss 4.15  | Δw: 262.929\n",
      "it: 6370  | loss 4.07  | Δw: 266.821\n",
      "it: 6380  | loss 4.09  | Δw: 272.311\n",
      "it: 6390  | loss 4.19  | Δw: 261.863\n",
      "it: 6400  | loss 4.23  | Δw: 267.269\n",
      "it: 6410  | loss 4.25  | Δw: 268.267\n",
      "it: 6420  | loss 4.05  | Δw: 258.539\n",
      "it: 6430  | loss 4.1  | Δw: 265.289\n",
      "it: 6440  | loss 4.05  | Δw: 265.673\n",
      "it: 6450  | loss 4.15  | Δw: 268.742\n",
      "it: 6460  | loss 4.09  | Δw: 269.978\n",
      "it: 6470  | loss 4.14  | Δw: 262.058\n",
      "it: 6480  | loss 4.2  | Δw: 263.56\n",
      "it: 6490  | loss 4.07  | Δw: 274.797\n",
      "it: 6500  | loss 4.1  | Δw: 261.513\n",
      "it: 6510  | loss 4.12  | Δw: 269.45\n",
      "it: 6520  | loss 4.19  | Δw: 265.003\n",
      "it: 6530  | loss 4.1  | Δw: 272.275\n",
      "it: 6540  | loss 4.26  | Δw: 262.29\n",
      "it: 6550  | loss 4.19  | Δw: 269.55\n",
      "it: 6560  | loss 4.06  | Δw: 268.328\n",
      "it: 6570  | loss 4.06  | Δw: 275.275\n",
      "it: 6580  | loss 4.11  | Δw: 270.849\n",
      "it: 6590  | loss 4.13  | Δw: 262.786\n",
      "it: 6600  | loss 4.15  | Δw: 265.346\n",
      "it: 6610  | loss 4.21  | Δw: 265.329\n",
      "it: 6620  | loss 4.06  | Δw: 261.541\n",
      "it: 6630  | loss 4.13  | Δw: 278.436\n",
      "it: 6640  | loss 4.12  | Δw: 269.333\n",
      "it: 6650  | loss 4.1  | Δw: 272.536\n",
      "it: 6660  | loss 4.05  | Δw: 266.731\n",
      "it: 6670  | loss 4.14  | Δw: 270.407\n",
      "it: 6680  | loss 4.13  | Δw: 275.249\n",
      "it: 6690  | loss 4.18  | Δw: 272.673\n",
      "it: 6700  | loss 4.11  | Δw: 277.931\n",
      "it: 6710  | loss 4.12  | Δw: 274.67\n",
      "it: 6720  | loss 4.25  | Δw: 269.584\n",
      "it: 6730  | loss 4.14  | Δw: 276.622\n",
      "it: 6740  | loss 4.06  | Δw: 268.115\n",
      "it: 6750  | loss 4.21  | Δw: 282.471\n",
      "it: 6760  | loss 4.11  | Δw: 264.502\n",
      "it: 6770  | loss 4.24  | Δw: 268.079\n",
      "it: 6780  | loss 4.02  | Δw: 267.039\n",
      "it: 6790  | loss 4.04  | Δw: 270.225\n",
      "it: 6800  | loss 4.05  | Δw: 279.346\n",
      "it: 6810  | loss 4.16  | Δw: 276.839\n",
      "it: 6820  | loss 4.06  | Δw: 273.17\n",
      "it: 6830  | loss 4.03  | Δw: 270.046\n",
      "it: 6840  | loss 4.12  | Δw: 274.664\n",
      "it: 6850  | loss 4.04  | Δw: 273.47\n",
      "it: 6860  | loss 4.08  | Δw: 261.653\n",
      "it: 6870  | loss 4.16  | Δw: 263.712\n",
      "it: 6880  | loss 4.15  | Δw: 275.004\n",
      "it: 6890  | loss 4.08  | Δw: 269.922\n",
      "it: 6900  | loss 4.14  | Δw: 266.92\n",
      "it: 6910  | loss 4.11  | Δw: 281.497\n",
      "it: 6920  | loss 4.08  | Δw: 272.321\n",
      "it: 6930  | loss 4.07  | Δw: 272.129\n",
      "it: 6940  | loss 4.0  | Δw: 274.017\n",
      "it: 6950  | loss 4.06  | Δw: 276.605\n",
      "it: 6960  | loss 4.08  | Δw: 268.468\n",
      "it: 6970  | loss 4.13  | Δw: 266.014\n",
      "it: 6980  | loss 4.2  | Δw: 271.462\n",
      "it: 6990  | loss 4.07  | Δw: 283.634\n",
      "it: 7000  | loss 4.09  | Δw: 279.569\n",
      "it: 7010  | loss 4.13  | Δw: 269.327\n",
      "it: 7020  | loss 4.04  | Δw: 273.487\n",
      "it: 7030  | loss 4.11  | Δw: 272.914\n",
      "it: 7040  | loss 3.91  | Δw: 274.987\n",
      "it: 7050  | loss 4.05  | Δw: 266.413\n",
      "it: 7060  | loss 4.18  | Δw: 274.484\n",
      "it: 7070  | loss 4.02  | Δw: 270.687\n",
      "it: 7080  | loss 4.05  | Δw: 276.207\n",
      "it: 7090  | loss 3.96  | Δw: 278.124\n",
      "it: 7100  | loss 4.01  | Δw: 276.644\n",
      "it: 7110  | loss 4.1  | Δw: 277.122\n",
      "it: 7120  | loss 4.09  | Δw: 272.565\n",
      "it: 7130  | loss 4.03  | Δw: 269.552\n",
      "it: 7140  | loss 4.11  | Δw: 273.299\n",
      "it: 7150  | loss 4.05  | Δw: 274.695\n",
      "it: 7160  | loss 4.04  | Δw: 273.244\n",
      "it: 7170  | loss 4.09  | Δw: 271.291\n",
      "it: 7180  | loss 3.95  | Δw: 290.179\n",
      "it: 7190  | loss 4.04  | Δw: 277.807\n",
      "it: 7200  | loss 4.02  | Δw: 279.921\n",
      "it: 7210  | loss 4.05  | Δw: 279.818\n",
      "it: 7220  | loss 4.09  | Δw: 276.887\n",
      "it: 7230  | loss 4.11  | Δw: 282.244\n",
      "it: 7240  | loss 4.15  | Δw: 275.411\n",
      "it: 7250  | loss 3.97  | Δw: 286.13\n",
      "it: 7260  | loss 4.1  | Δw: 269.77\n",
      "it: 7270  | loss 4.01  | Δw: 292.188\n",
      "it: 7280  | loss 4.03  | Δw: 274.565\n",
      "it: 7290  | loss 4.02  | Δw: 274.423\n",
      "it: 7300  | loss 4.05  | Δw: 272.291\n",
      "it: 7310  | loss 4.0  | Δw: 278.818\n",
      "it: 7320  | loss 4.1  | Δw: 275.371\n",
      "it: 7330  | loss 3.98  | Δw: 268.628\n",
      "it: 7340  | loss 4.05  | Δw: 277.389\n",
      "it: 7350  | loss 4.1  | Δw: 278.36\n",
      "it: 7360  | loss 4.15  | Δw: 281.381\n",
      "it: 7370  | loss 4.17  | Δw: 289.302\n",
      "it: 7380  | loss 4.03  | Δw: 274.809\n",
      "it: 7390  | loss 4.0  | Δw: 272.988\n",
      "it: 7400  | loss 3.97  | Δw: 277.75\n",
      "it: 7410  | loss 4.05  | Δw: 278.29\n",
      "it: 7420  | loss 3.97  | Δw: 272.948\n",
      "it: 7430  | loss 4.07  | Δw: 272.458\n",
      "it: 7440  | loss 4.03  | Δw: 273.821\n",
      "it: 7450  | loss 4.0  | Δw: 279.367\n",
      "it: 7460  | loss 4.0  | Δw: 264.334\n",
      "it: 7470  | loss 4.03  | Δw: 279.693\n",
      "it: 7480  | loss 3.99  | Δw: 278.55\n",
      "it: 7490  | loss 4.1  | Δw: 274.976\n",
      "it: 7500  | loss 4.12  | Δw: 271.874\n",
      "it: 7510  | loss 4.06  | Δw: 282.033\n",
      "it: 7520  | loss 4.11  | Δw: 278.918\n",
      "it: 7530  | loss 4.05  | Δw: 272.868\n",
      "it: 7540  | loss 3.99  | Δw: 271.586\n",
      "it: 7550  | loss 4.01  | Δw: 280.143\n",
      "it: 7560  | loss 4.13  | Δw: 283.839\n",
      "it: 7570  | loss 3.94  | Δw: 280.662\n",
      "it: 7580  | loss 4.07  | Δw: 284.421\n",
      "it: 7590  | loss 4.04  | Δw: 281.459\n",
      "it: 7600  | loss 4.05  | Δw: 289.392\n",
      "it: 7610  | loss 4.08  | Δw: 270.922\n",
      "it: 7620  | loss 3.98  | Δw: 269.491\n",
      "it: 7630  | loss 3.98  | Δw: 277.462\n",
      "it: 7640  | loss 3.9  | Δw: 282.693\n",
      "it: 7650  | loss 3.9  | Δw: 274.988\n",
      "it: 7660  | loss 4.05  | Δw: 280.617\n",
      "it: 7670  | loss 4.0  | Δw: 286.761\n",
      "it: 7680  | loss 4.0  | Δw: 283.913\n",
      "it: 7690  | loss 4.04  | Δw: 276.595\n",
      "it: 7700  | loss 4.11  | Δw: 272.866\n",
      "it: 7710  | loss 4.02  | Δw: 271.578\n",
      "it: 7720  | loss 3.92  | Δw: 278.551\n",
      "it: 7730  | loss 3.96  | Δw: 276.164\n",
      "it: 7740  | loss 4.13  | Δw: 283.253\n",
      "it: 7750  | loss 4.07  | Δw: 282.079\n",
      "it: 7760  | loss 3.93  | Δw: 283.25\n",
      "it: 7770  | loss 4.09  | Δw: 282.243\n",
      "it: 7780  | loss 4.06  | Δw: 283.927\n",
      "it: 7790  | loss 4.04  | Δw: 276.761\n",
      "it: 7800  | loss 4.05  | Δw: 286.216\n",
      "it: 7810  | loss 4.0  | Δw: 288.349\n",
      "it: 7820  | loss 3.99  | Δw: 280.098\n",
      "it: 7830  | loss 3.96  | Δw: 281.979\n",
      "it: 7840  | loss 3.97  | Δw: 281.783\n",
      "it: 7850  | loss 3.91  | Δw: 279.093\n",
      "it: 7860  | loss 3.97  | Δw: 291.075\n",
      "it: 7870  | loss 4.07  | Δw: 282.436\n",
      "it: 7880  | loss 3.98  | Δw: 279.276\n",
      "it: 7890  | loss 4.11  | Δw: 288.682\n",
      "it: 7900  | loss 4.07  | Δw: 283.264\n",
      "it: 7910  | loss 3.99  | Δw: 283.685\n",
      "it: 7920  | loss 4.04  | Δw: 278.004\n",
      "it: 7930  | loss 3.93  | Δw: 281.175\n",
      "it: 7940  | loss 3.96  | Δw: 284.6\n",
      "it: 7950  | loss 3.85  | Δw: 276.308\n",
      "it: 7960  | loss 4.0  | Δw: 294.16\n",
      "it: 7970  | loss 3.86  | Δw: 269.336\n",
      "it: 7980  | loss 3.95  | Δw: 270.924\n",
      "it: 7990  | loss 3.88  | Δw: 284.93\n",
      "it: 8000  | loss 3.95  | Δw: 280.688\n",
      "it: 8010  | loss 4.01  | Δw: 281.319\n",
      "it: 8020  | loss 4.02  | Δw: 280.124\n",
      "it: 8030  | loss 3.88  | Δw: 286.841\n",
      "it: 8040  | loss 3.95  | Δw: 288.93\n",
      "it: 8050  | loss 3.91  | Δw: 289.759\n",
      "it: 8060  | loss 3.91  | Δw: 271.199\n",
      "it: 8070  | loss 3.96  | Δw: 276.133\n",
      "it: 8080  | loss 3.91  | Δw: 283.955\n",
      "it: 8090  | loss 3.89  | Δw: 287.12\n",
      "it: 8100  | loss 3.95  | Δw: 289.568\n",
      "it: 8110  | loss 3.87  | Δw: 291.037\n",
      "it: 8120  | loss 3.9  | Δw: 290.902\n",
      "it: 8130  | loss 3.77  | Δw: 287.076\n",
      "it: 8140  | loss 3.87  | Δw: 287.24\n",
      "it: 8150  | loss 4.04  | Δw: 286.6\n",
      "it: 8160  | loss 3.84  | Δw: 282.456\n",
      "it: 8170  | loss 4.03  | Δw: 290.382\n",
      "it: 8180  | loss 4.0  | Δw: 289.053\n",
      "it: 8190  | loss 3.91  | Δw: 287.088\n",
      "it: 8200  | loss 4.02  | Δw: 283.05\n",
      "it: 8210  | loss 4.0  | Δw: 281.72\n",
      "it: 8220  | loss 3.94  | Δw: 298.619\n",
      "it: 8230  | loss 3.87  | Δw: 283.422\n",
      "it: 8240  | loss 3.9  | Δw: 286.796\n",
      "it: 8250  | loss 4.0  | Δw: 293.516\n",
      "it: 8260  | loss 4.0  | Δw: 284.568\n",
      "it: 8270  | loss 3.91  | Δw: 287.844\n",
      "it: 8280  | loss 3.91  | Δw: 290.493\n",
      "it: 8290  | loss 3.98  | Δw: 280.939\n",
      "it: 8300  | loss 3.95  | Δw: 279.961\n",
      "it: 8310  | loss 3.93  | Δw: 294.71\n",
      "it: 8320  | loss 3.96  | Δw: 284.784\n",
      "it: 8330  | loss 3.93  | Δw: 290.249\n",
      "it: 8340  | loss 3.95  | Δw: 289.398\n",
      "it: 8350  | loss 3.79  | Δw: 284.722\n",
      "it: 8360  | loss 3.99  | Δw: 285.468\n",
      "it: 8370  | loss 3.82  | Δw: 287.905\n",
      "it: 8380  | loss 3.97  | Δw: 288.308\n",
      "it: 8390  | loss 3.88  | Δw: 285.251\n",
      "it: 8400  | loss 3.96  | Δw: 292.713\n",
      "it: 8410  | loss 3.87  | Δw: 291.139\n",
      "it: 8420  | loss 3.95  | Δw: 293.079\n",
      "it: 8430  | loss 3.91  | Δw: 291.714\n",
      "it: 8440  | loss 3.84  | Δw: 292.974\n",
      "it: 8450  | loss 3.91  | Δw: 281.093\n",
      "it: 8460  | loss 3.94  | Δw: 287.734\n",
      "it: 8470  | loss 3.91  | Δw: 297.12\n",
      "it: 8480  | loss 3.74  | Δw: 286.629\n",
      "it: 8490  | loss 3.8  | Δw: 304.132\n",
      "it: 8500  | loss 3.91  | Δw: 290.08\n",
      "it: 8510  | loss 3.77  | Δw: 282.851\n",
      "it: 8520  | loss 3.86  | Δw: 284.353\n",
      "it: 8530  | loss 3.98  | Δw: 295.616\n",
      "it: 8540  | loss 3.8  | Δw: 277.233\n",
      "it: 8550  | loss 4.02  | Δw: 289.094\n",
      "it: 8560  | loss 4.0  | Δw: 294.892\n",
      "it: 8570  | loss 3.94  | Δw: 289.136\n",
      "it: 8580  | loss 3.86  | Δw: 277.845\n",
      "it: 8590  | loss 3.97  | Δw: 290.477\n",
      "it: 8600  | loss 3.92  | Δw: 279.842\n",
      "it: 8610  | loss 3.96  | Δw: 293.771\n",
      "it: 8620  | loss 3.82  | Δw: 301.675\n",
      "it: 8630  | loss 3.89  | Δw: 293.776\n",
      "it: 8640  | loss 3.87  | Δw: 292.213\n",
      "it: 8650  | loss 3.96  | Δw: 277.468\n",
      "it: 8660  | loss 3.95  | Δw: 291.974\n",
      "it: 8670  | loss 3.9  | Δw: 295.034\n",
      "it: 8680  | loss 3.96  | Δw: 292.866\n",
      "it: 8690  | loss 4.03  | Δw: 285.336\n",
      "it: 8700  | loss 3.85  | Δw: 293.14\n",
      "it: 8710  | loss 3.96  | Δw: 288.721\n",
      "it: 8720  | loss 3.92  | Δw: 285.881\n",
      "it: 8730  | loss 3.91  | Δw: 295.351\n",
      "it: 8740  | loss 3.93  | Δw: 293.64\n",
      "it: 8750  | loss 3.95  | Δw: 295.916\n",
      "it: 8760  | loss 3.85  | Δw: 296.846\n",
      "it: 8770  | loss 3.88  | Δw: 296.202\n",
      "it: 8780  | loss 3.76  | Δw: 287.05\n",
      "it: 8790  | loss 3.81  | Δw: 285.826\n",
      "it: 8800  | loss 4.02  | Δw: 289.551\n",
      "it: 8810  | loss 3.91  | Δw: 286.509\n",
      "it: 8820  | loss 3.92  | Δw: 290.619\n",
      "it: 8830  | loss 3.89  | Δw: 289.267\n",
      "it: 8840  | loss 3.85  | Δw: 297.627\n",
      "it: 8850  | loss 3.85  | Δw: 299.696\n",
      "it: 8860  | loss 3.83  | Δw: 293.746\n",
      "it: 8870  | loss 3.83  | Δw: 280.849\n",
      "it: 8880  | loss 3.93  | Δw: 294.611\n",
      "it: 8890  | loss 3.97  | Δw: 290.45\n",
      "it: 8900  | loss 3.86  | Δw: 301.343\n",
      "it: 8910  | loss 3.84  | Δw: 284.556\n",
      "it: 8920  | loss 3.84  | Δw: 286.918\n",
      "it: 8930  | loss 3.86  | Δw: 293.279\n",
      "it: 8940  | loss 3.8  | Δw: 283.789\n",
      "it: 8950  | loss 3.84  | Δw: 307.333\n",
      "it: 8960  | loss 3.83  | Δw: 282.362\n",
      "it: 8970  | loss 3.85  | Δw: 292.612\n",
      "it: 8980  | loss 3.86  | Δw: 294.687\n",
      "it: 8990  | loss 3.97  | Δw: 294.102\n",
      "it: 9000  | loss 3.89  | Δw: 291.258\n",
      "it: 9010  | loss 3.75  | Δw: 292.598\n",
      "it: 9020  | loss 3.79  | Δw: 293.525\n",
      "it: 9030  | loss 3.95  | Δw: 291.764\n",
      "it: 9040  | loss 3.75  | Δw: 286.902\n",
      "it: 9050  | loss 3.93  | Δw: 282.808\n",
      "it: 9060  | loss 3.83  | Δw: 284.99\n",
      "it: 9070  | loss 3.97  | Δw: 301.582\n",
      "it: 9080  | loss 3.9  | Δw: 292.281\n",
      "it: 9090  | loss 3.88  | Δw: 295.249\n",
      "it: 9100  | loss 3.83  | Δw: 288.283\n",
      "it: 9110  | loss 3.89  | Δw: 303.573\n",
      "it: 9120  | loss 3.9  | Δw: 304.253\n",
      "it: 9130  | loss 3.93  | Δw: 297.786\n",
      "it: 9140  | loss 3.82  | Δw: 286.693\n",
      "it: 9150  | loss 3.81  | Δw: 294.034\n",
      "it: 9160  | loss 3.9  | Δw: 292.726\n",
      "it: 9170  | loss 3.9  | Δw: 298.11\n",
      "it: 9180  | loss 3.96  | Δw: 310.576\n",
      "it: 9190  | loss 3.87  | Δw: 284.261\n",
      "it: 9200  | loss 3.9  | Δw: 291.606\n",
      "it: 9210  | loss 3.89  | Δw: 306.954\n",
      "it: 9220  | loss 3.83  | Δw: 295.454\n",
      "it: 9230  | loss 3.92  | Δw: 289.222\n",
      "it: 9240  | loss 3.84  | Δw: 289.807\n",
      "it: 9250  | loss 3.86  | Δw: 296.701\n",
      "it: 9260  | loss 3.96  | Δw: 292.88\n",
      "it: 9270  | loss 3.88  | Δw: 286.231\n",
      "it: 9280  | loss 3.83  | Δw: 289.577\n",
      "it: 9290  | loss 3.87  | Δw: 290.289\n",
      "it: 9300  | loss 3.78  | Δw: 296.428\n",
      "it: 9310  | loss 3.79  | Δw: 288.643\n",
      "it: 9320  | loss 3.94  | Δw: 290.569\n",
      "it: 9330  | loss 3.78  | Δw: 297.466\n",
      "it: 9340  | loss 3.83  | Δw: 292.522\n",
      "it: 9350  | loss 3.88  | Δw: 297.924\n",
      "it: 9360  | loss 3.86  | Δw: 295.79\n",
      "it: 9370  | loss 3.78  | Δw: 304.095\n",
      "it: 9380  | loss 3.72  | Δw: 287.798\n",
      "it: 9390  | loss 3.81  | Δw: 291.5\n",
      "it: 9400  | loss 4.01  | Δw: 295.951\n",
      "it: 9410  | loss 3.86  | Δw: 299.514\n",
      "it: 9420  | loss 3.85  | Δw: 294.227\n",
      "it: 9430  | loss 3.95  | Δw: 303.1\n",
      "it: 9440  | loss 3.84  | Δw: 302.633\n",
      "it: 9450  | loss 3.86  | Δw: 291.866\n",
      "it: 9460  | loss 3.76  | Δw: 290.854\n",
      "it: 9470  | loss 4.03  | Δw: 292.273\n",
      "it: 9480  | loss 3.78  | Δw: 307.724\n",
      "it: 9490  | loss 3.81  | Δw: 292.793\n",
      "it: 9500  | loss 3.94  | Δw: 303.058\n",
      "it: 9510  | loss 3.81  | Δw: 286.362\n",
      "it: 9520  | loss 3.68  | Δw: 290.053\n",
      "it: 9530  | loss 3.77  | Δw: 290.863\n",
      "it: 9540  | loss 3.83  | Δw: 296.668\n",
      "it: 9550  | loss 3.83  | Δw: 304.104\n",
      "it: 9560  | loss 3.77  | Δw: 298.741\n",
      "it: 9570  | loss 3.78  | Δw: 290.91\n",
      "it: 9580  | loss 3.89  | Δw: 305.142\n",
      "it: 9590  | loss 3.86  | Δw: 294.79\n",
      "it: 9600  | loss 3.75  | Δw: 302.462\n",
      "it: 9610  | loss 3.91  | Δw: 308.666\n",
      "it: 9620  | loss 3.87  | Δw: 303.347\n",
      "it: 9630  | loss 3.89  | Δw: 292.217\n",
      "it: 9640  | loss 3.91  | Δw: 301.732\n",
      "it: 9650  | loss 3.86  | Δw: 303.56\n",
      "it: 9660  | loss 3.99  | Δw: 298.358\n",
      "it: 9670  | loss 3.87  | Δw: 305.003\n",
      "it: 9680  | loss 3.82  | Δw: 289.672\n",
      "it: 9690  | loss 3.92  | Δw: 299.554\n",
      "it: 9700  | loss 3.81  | Δw: 297.627\n",
      "it: 9710  | loss 3.8  | Δw: 298.612\n",
      "it: 9720  | loss 3.77  | Δw: 295.983\n",
      "it: 9730  | loss 3.79  | Δw: 296.112\n",
      "it: 9740  | loss 3.88  | Δw: 303.377\n",
      "it: 9750  | loss 3.83  | Δw: 293.638\n",
      "it: 9760  | loss 3.76  | Δw: 286.715\n",
      "it: 9770  | loss 3.81  | Δw: 303.219\n",
      "it: 9780  | loss 3.87  | Δw: 307.1\n",
      "it: 9790  | loss 3.88  | Δw: 300.749\n",
      "it: 9800  | loss 3.78  | Δw: 293.157\n",
      "it: 9810  | loss 3.8  | Δw: 294.381\n",
      "it: 9820  | loss 3.76  | Δw: 308.646\n",
      "it: 9830  | loss 3.85  | Δw: 297.386\n",
      "it: 9840  | loss 3.72  | Δw: 296.255\n",
      "it: 9850  | loss 3.65  | Δw: 300.364\n",
      "it: 9860  | loss 3.89  | Δw: 302.855\n",
      "it: 9870  | loss 3.75  | Δw: 293.949\n",
      "it: 9880  | loss 3.77  | Δw: 303.514\n",
      "it: 9890  | loss 3.72  | Δw: 301.011\n",
      "it: 9900  | loss 3.77  | Δw: 291.368\n",
      "it: 9910  | loss 3.73  | Δw: 298.82\n",
      "it: 9920  | loss 3.81  | Δw: 297.498\n",
      "it: 9930  | loss 3.77  | Δw: 299.555\n",
      "it: 9940  | loss 3.83  | Δw: 303.293\n",
      "it: 9950  | loss 3.85  | Δw: 307.853\n",
      "it: 9960  | loss 3.79  | Δw: 302.65\n",
      "it: 9970  | loss 3.77  | Δw: 307.317\n",
      "it: 9980  | loss 3.78  | Δw: 312.835\n",
      "it: 9990  | loss 3.8  | Δw: 296.53\n"
     ]
    }
   ],
   "source": [
    "model = build_BERT(N=n_code, n_heads=n_heads, d_model=embed_size, d_ff=inner_ff_size, src_vocab_size=len(dataset.vocab), src_seq_len=seq_len, dropout=dropout, tgt_vocab_size=len(dataset.vocab))\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 10000\n",
    "for it in range(n_iteration):\n",
    "\n",
    "    # get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "\n",
    "    # infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "\n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "\n",
    "    # compute the cross entropy loss\n",
    "    output_v = output.view(-1, output.shape[-1])\n",
    "    target_v = masked_target.view(-1, 1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # apply gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it,\n",
    "              ' | loss', np.round(loss.item(), 2),\n",
    "              ' | Δw:', round(model.src_embed.embedding.weight.grad.abs().sum().item(), 3))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:41:25.938291058Z",
     "start_time": "2023-09-15T19:12:17.302895393Z"
    }
   },
   "id": "56a84cd843e51dee"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving embeddings...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BERT' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msaving embeddings...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m N \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3000\u001B[39m\n\u001B[0;32m----> 6\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalues.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, np\u001B[38;5;241m.\u001B[39mround(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()[\u001B[38;5;241m0\u001B[39m:N], \u001B[38;5;241m2\u001B[39m), delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%1.2f\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m s \u001B[38;5;241m=\u001B[39m [dataset\u001B[38;5;241m.\u001B[39mrvocab[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(N)]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnames.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw+\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(s))\n",
      "File \u001B[0;32m~/MyWork/EVA/ERA1/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1612\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1613\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1614\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BERT' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.src_embed.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s))\n",
    "\n",
    "print('end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T19:41:26.613384694Z",
     "start_time": "2023-09-15T19:41:25.943653758Z"
    }
   },
   "id": "57a45f3bf827a915"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
